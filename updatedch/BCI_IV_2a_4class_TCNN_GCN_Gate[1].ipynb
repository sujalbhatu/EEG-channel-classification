{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe61410",
   "metadata": {},
   "source": [
    "\n",
    "# BCI Competition IV-2a — 4‑Class Motor Imagery with TemporalCNN + GCN + Channel Gating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa93b1",
   "metadata": {},
   "source": [
    "\n",
    "This notebook fixes the *\"all predictions collapse to class 0\"* issue by **properly loading all four MI classes** (Left, Right, Foot, Tongue) and building a **4-way classifier**.  \n",
    "It uses a hybrid model:\n",
    "- Temporal 1D CNN per channel → time pooling (extracts band‑limited features)\n",
    "- **Channel Gating** (learnable per‑channel weights; L1 sparsity to encourage selection)\n",
    "- **Graph Convolution** (GCN) over channels using an adjacency built from electrode distances\n",
    "- MLP classifier (4 classes)\n",
    "\n",
    "You’ll get:\n",
    "- Robust **4‑class event mapping** (supports 769–772, 1–4, `'T1'..'T4'`, or textual names)\n",
    "- Clean train/val/test split (session‑aware or stratified)\n",
    "- Training loop with early stopping & scheduler\n",
    "- Metrics: accuracy, macro‑F1, Cohen’s kappa, per‑class report\n",
    "- **Confusion matrix** and **Top‑k channels** from the learned gate\n",
    "\n",
    "> **Dataset**: BCI Competition IV‑2a (GDF files) — put them under `DATA_ROOT/subjectXX/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "855d9802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied np.fromstring compatibility shim. numpy version: 2.3.3\n",
      "test bytes => [ 92 120  48  49  92 120  48  50]\n",
      "test str   => [1 2]\n"
     ]
    }
   ],
   "source": [
    "# Compatibility shim for MNE GDF header reading with NumPy 2.x\n",
    "# Paste this at the top of the notebook (before importing mne or reading GDFs).\n",
    "import numpy as np\n",
    "\n",
    "# keep original so we can still use text-mode path\n",
    "_orig_fromstring = np.fromstring\n",
    "\n",
    "def _fromstring_compat(data, dtype=float, sep=''):\n",
    "    \"\"\"\n",
    "    Compatibility wrapper:\n",
    "      - If sep=='' (binary-mode expected) and data is bytes/bytearray -> use frombuffer\n",
    "      - If sep=='' and data is str -> encode with latin-1 (one-to-one) then frombuffer\n",
    "      - Otherwise call original np.fromstring (text parsing)\n",
    "    \"\"\"\n",
    "    if sep == '':\n",
    "        # binary/path expected\n",
    "        if isinstance(data, (bytes, bytearray)):\n",
    "            return np.frombuffer(data, dtype=dtype)\n",
    "        if isinstance(data, str):\n",
    "            # map characters to original byte values (latin-1 is 1:1 for 0-255)\n",
    "            try:\n",
    "                b = data.encode('latin-1')\n",
    "            except Exception:\n",
    "                # fallback: preserve raw bytes via surrogatepass if weird unicode present\n",
    "                b = data.encode('utf-8', errors='surrogatepass')\n",
    "            return np.frombuffer(b, dtype=dtype)\n",
    "    # text mode -> delegate to original behaviour (parses str with sep)\n",
    "    return _orig_fromstring(data, dtype=dtype, sep=sep)\n",
    "\n",
    "# apply patch\n",
    "np.fromstring = _fromstring_compat\n",
    "\n",
    "print(\"Applied np.fromstring compatibility shim. numpy version:\", np.__version__)\n",
    "# quick self-test (should not error)\n",
    "print(\"test bytes =>\", np.fromstring(b'\\\\x01\\\\x02', dtype=np.uint8))\n",
    "print(\"test str   =>\", np.fromstring('\\x01\\x02', dtype=np.uint8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f59bf",
   "metadata": {},
   "source": [
    "## 1. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f9c125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0+cpu | MNE: 1.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %pip install -q mne numpy scipy scikit-learn torch==2.2.0 torchaudio torchvision matplotlib\n",
    "import os, re, math, random, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import mne\n",
    "from scipy import signal\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, cohen_kappa_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "print(\"Torch:\", torch.__version__, \"| MNE:\", mne.__version__)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d2af7",
   "metadata": {},
   "source": [
    "## 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4867b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_ROOT = Path(\"./../BCICIV_2a_gdf\")  # <-- CHANGE THIS\n",
    "SAVE_DIR = Path(\"./outputs_bci2a_gcn\"); SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SF_TARGET = 250; BANDPASS = (4., 38.); NOTCH = 50.0\n",
    "TMIN = 0.5; TMAX = 3.5; REF = \"average\"\n",
    "N_EPOCHS = 60; BATCH_SIZE = 64; LR = 1e-3; WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 12; LR_PATIENCE = 6; GATE_L1 = 1e-4\n",
    "SESSION_A_TRAIN_SESSION_B_TEST = True; VAL_SIZE = 0.15\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED); random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED); torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "BCI2A_CHS = [\"Fz\",\"FC3\",\"FC1\",\"FCz\",\"FC2\",\"FC4\",\"C5\",\"C3\",\"C1\",\"Cz\",\"C2\",\"C4\",\"C6\",\"CP3\",\"CP1\",\"CPz\",\"CP2\",\"CP4\",\"P1\",\"Pz\",\"P2\",\"POz\"]\n",
    "CLASS_NAMES = [\"Left\", \"Right\", \"Foot\", \"Tongue\"]; NUM_CLASSES = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb974d",
   "metadata": {},
   "source": [
    "## 3. Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8701397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed=RANDOM_SEED):\n",
    "    np.random.seed(seed); random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "def print_counts(prefix, y):\n",
    "    counts = np.bincount(y, minlength=NUM_CLASSES)\n",
    "    print(prefix, dict(zip(CLASS_NAMES, counts.tolist())))\n",
    "set_seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace95fe5",
   "metadata": {},
   "source": [
    "## 4. Event Mapping (robust to different encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19556b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_event_map(event_id_dict, raw=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Create a mapping: numeric_event_code (the ints you see as ev[2]) -> label 0..3\n",
    "    Heuristics:\n",
    "     - If annotation keys include textual tags (t1/t2/t3/t4 or left/right/foot/tongue),\n",
    "       map accordingly.\n",
    "     - If annotation keys are numeric-strings and match common sets (1..4 or 769..772), use canonical mapping.\n",
    "     - Otherwise, if raw is provided, pick the top-4 most frequent annotation descriptions (excluding sentinel/boundary codes)\n",
    "       and map them to 0..3 in frequency order (prints mapping so you can inspect).\n",
    "    \"\"\"\n",
    "    # Normalize keys -> strings\n",
    "    ev_items = {str(k): int(v) for k, v in event_id_dict.items()}\n",
    "    keys = list(ev_items.keys())\n",
    "    if verbose:\n",
    "        print(\"build_event_map: annotation keys (sample):\", keys[:12])\n",
    "\n",
    "    # 1) textual mapping: try to map 't1','left','right','foot','tongue'\n",
    "    textual_map = {}\n",
    "    for desc, code in ev_items.items():\n",
    "        s = desc.lower()\n",
    "        if \"t1\" in s or \"left\" in s:\n",
    "            textual_map[code] = 0\n",
    "        elif \"t2\" in s or \"right\" in s:\n",
    "            textual_map[code] = 1\n",
    "        elif \"t3\" in s or \"foot\" in s or \"leg\" in s:\n",
    "            textual_map[code] = 2\n",
    "        elif \"t4\" in s or \"tongue\" in s:\n",
    "            textual_map[code] = 3\n",
    "    if len(textual_map) == 4:\n",
    "        if verbose: print(\"Detected textual mapping:\", textual_map)\n",
    "        return textual_map\n",
    "\n",
    "    # 2) numeric-string keys -> try canonical code sets\n",
    "    num_desc_to_code = {}\n",
    "    for desc, code in ev_items.items():\n",
    "        try:\n",
    "            dnum = int(desc)\n",
    "            num_desc_to_code[dnum] = code\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # try known sets\n",
    "    if set([769,770,771,772]).issubset(set(num_desc_to_code.keys())):\n",
    "        mapping = {\n",
    "            num_desc_to_code[769]: 0,\n",
    "            num_desc_to_code[770]: 1,\n",
    "            num_desc_to_code[771]: 2,\n",
    "            num_desc_to_code[772]: 3,\n",
    "        }\n",
    "        if verbose: print(\"Detected BCI2000(769..772) numeric mapping ->\", mapping)\n",
    "        return mapping\n",
    "\n",
    "    if set([1,2,3,4]).issubset(set(num_desc_to_code.keys())):\n",
    "        mapping = {\n",
    "            num_desc_to_code[1]: 0,\n",
    "            num_desc_to_code[2]: 1,\n",
    "            num_desc_to_code[3]: 2,\n",
    "            num_desc_to_code[4]: 3,\n",
    "        }\n",
    "        if verbose: print(\"Detected simple (1..4) numeric mapping ->\", mapping)\n",
    "        return mapping\n",
    "\n",
    "    # 3) fallback: use annotation frequencies from raw.annotations (if raw provided)\n",
    "    if raw is not None:\n",
    "        descs = list(raw.annotations.description)\n",
    "        if len(descs) == 0:\n",
    "            raise RuntimeError(\"No annotations available in raw.annotations to auto-detect event mapping.\")\n",
    "        c = Counter(descs)\n",
    "        # filter candidates to only annotations present in event_id_dict\n",
    "        candidates = {desc: cnt for desc, cnt in c.items() if desc in ev_items}\n",
    "        # filter-out likely sentinel/boundary codes:\n",
    "        def is_unwanted(desc):\n",
    "            s = str(desc).lower()\n",
    "            if any(x in s for x in (\"bad\", \"boundary\", \"artifact\", \"start\", \"stop\")):\n",
    "                return True\n",
    "            # if desc is numeric and very large (typical sentinel like 32766), ignore\n",
    "            try:\n",
    "                v = int(desc)\n",
    "                if v > 30000: \n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            return False\n",
    "\n",
    "        for bad in list(candidates.keys()):\n",
    "            if is_unwanted(bad):\n",
    "                candidates.pop(bad, None)\n",
    "\n",
    "        if len(candidates) >= 4:\n",
    "            top4 = [d for d, _ in sorted(candidates.items(), key=lambda x:-x[1])[:4]]\n",
    "            mapping = { ev_items[desc] : idx for idx, desc in enumerate(top4) }\n",
    "            if verbose:\n",
    "                print(\"Auto-mapped top-4 annotation descriptions -> labels (code -> label):\")\n",
    "                for code, lab in mapping.items():\n",
    "                    print(\"   code\", code, \" = label\", lab, \" (desc='{}', count={})\".format(\n",
    "                        [d for d in top4 if ev_items[d]==code][0], c[[d for d in top4 if ev_items[d]==code][0]]\n",
    "                    ))\n",
    "                print(\"Top4 desc order used:\", top4)\n",
    "            return mapping\n",
    "\n",
    "    # If none of the above worked — raise with helpful debug info\n",
    "    raise RuntimeError(f\"Could not build 4-class event map automatically. event_id keys: {list(ev_items.keys())}. \"\n",
    "                       \"If you know the mapping, pass a manual map or inspect raw.annotations.description and event_id. \"\n",
    "                       \"Example: event_id = \" + str(event_id_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185f1e7",
   "metadata": {},
   "source": [
    "## 5. Load & Preprocess BCI IV‑2a (one subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e2872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _find_subject_files(data_root: Path, subject_id: int):\n",
    "    s=f\"{subject_id:02d}\"\n",
    "    cand=list((data_root).rglob(f\"*{s}*.[gG][dD][fF]\"))\n",
    "    train_files=[p for p in cand if re.search(r\"(T|train)\", p.name, re.I)]\n",
    "    eval_files=[p for p in cand if re.search(r\"(E|eval)\", p.name, re.I)]\n",
    "    if not train_files or not eval_files: return cand, []\n",
    "    return train_files, eval_files\n",
    "\n",
    "def _get_montage_info(raw):\n",
    "    try:\n",
    "        montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "        raw.set_montage(montage, on_missing=\"ignore\")\n",
    "    except Exception as e:\n",
    "        print(\"Montage set failed (non-fatal):\", e)\n",
    "\n",
    "def _preprocess_raw(raw):\n",
    "    if REF==\"average\": raw.set_eeg_reference(\"average\", projection=False, verbose=False)\n",
    "    if NOTCH is not None: raw.notch_filter(freqs=[NOTCH], picks=\"eeg\", verbose=False)\n",
    "    raw.filter(BANDPASS[0], BANDPASS[1], picks=\"eeg\", verbose=False)\n",
    "    if SF_TARGET is not None and abs(raw.info[\"sfreq\"]-SF_TARGET)>1e-3:\n",
    "        raw.resample(SF_TARGET, npad=\"auto\", verbose=False)\n",
    "\n",
    "def _map_events(raw):\n",
    "    \"\"\"\n",
    "    Use mne.events_from_annotations and robustly map numeric event codes -> labels 0..3.\n",
    "    Returns events array (n,3) and emap (code->label).\n",
    "    \"\"\"\n",
    "    events, event_id = mne.events_from_annotations(raw, verbose=False)\n",
    "    print(\"mne.events_from_annotations -> event_id:\", event_id)\n",
    "    try:\n",
    "        emap = build_event_map(event_id, raw=raw, verbose=True)\n",
    "    except RuntimeError as e:\n",
    "        # raise with extra debug info\n",
    "        print(\"build_event_map failed; raw.annotations.description sample:\", raw.annotations.description[:30])\n",
    "        raise\n",
    "\n",
    "    # Build new events' label column (map numeric code -> 0..3); we will convert events accordingly\n",
    "    mapped_events = []\n",
    "    for ev in events:\n",
    "        onset, _, code = int(ev[0]), int(ev[1]) if ev.shape[1] > 1 else 0, int(ev[2])\n",
    "        if code not in emap:\n",
    "            # skip non-MI codes\n",
    "            continue\n",
    "        lbl = emap[code]\n",
    "        mapped_events.append([onset, 0, lbl])\n",
    "    if len(mapped_events) == 0:\n",
    "        raise RuntimeError(\"No MI events found after mapping — check annotations and event_id.\")\n",
    "    return np.array(mapped_events), emap\n",
    "\n",
    "def _epochs_from_raw(raw):\n",
    "    events, emap = _map_events(raw)\n",
    "    picks = mne.pick_types(raw.info, eeg=True, meg=False, stim=False, eog=False)\n",
    "    ch_names = [raw.ch_names[i] for i in picks]\n",
    "    keep_idx = [raw.ch_names.index(ch) for ch in BCI2A_CHS if ch in raw.ch_names]\n",
    "    if len(keep_idx)>=8: picks=keep_idx; ch_names=[raw.ch_names[i] for i in picks]\n",
    "    epochs = mne.Epochs(raw, events, event_id=None, tmin=TMIN, tmax=TMAX,\n",
    "                        picks=picks, baseline=None, preload=True, verbose=False)\n",
    "    X=epochs.get_data(); y=epochs.events[:,-1]\n",
    "    return X, y, ch_names, raw.info[\"sfreq\"]\n",
    "\n",
    "def load_subject(data_root: Path, subject_id: int):\n",
    "    train_files, eval_files = _find_subject_files(data_root, subject_id)\n",
    "    if not train_files: raise FileNotFoundError(f\"No GDF for S{subject_id:02d}\")\n",
    "    rawT_list=[]\n",
    "    for f in sorted(train_files):\n",
    "        raw=mne.io.read_raw_gdf(f, preload=True, verbose=False)\n",
    "        _get_montage_info(raw); _preprocess_raw(raw); rawT_list.append(raw)\n",
    "    rawT=mne.concatenate_raws(rawT_list, verbose=False)\n",
    "    Xtr,ytr,chs,sf=_epochs_from_raw(rawT)\n",
    "    Xte=yte=None\n",
    "    if eval_files:\n",
    "        rawE_list=[]\n",
    "        for f in sorted(eval_files):\n",
    "            raw=mne.io.read_raw_gdf(f, preload=True, verbose=False)\n",
    "            _get_montage_info(raw); _preprocess_raw(raw); rawE_list.append(raw)\n",
    "        rawE=mne.concatenate_raws(rawE_list, verbose=False)\n",
    "        Xte,yte,_,_= _epochs_from_raw(rawE)\n",
    "    return Xtr,ytr,Xte,yte,chs,sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c2d05a",
   "metadata": {},
   "source": [
    "## 6. Channel Graph (Adjacency from electrode distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72bbaafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_adjacency(ch_names, info, sigma=0.08, self_loop=True):\n",
    "    \"\"\"\n",
    "    Build adjacency using 3D positions from info['chs'][i]['loc'] (or montage).\n",
    "    A_ij = exp(-||p_i - p_j||^2 / (2 sigma^2)), then normalize with D^{-1/2} A D^{-1/2}.\n",
    "    \"\"\"\n",
    "    pos=[]; name_to_idx={n:i for i,n in enumerate(info[\"ch_names\"])}\n",
    "    for ch in ch_names:\n",
    "        if ch in info[\"ch_names\"]:\n",
    "            idx=name_to_idx[ch]; loc=info[\"chs\"][idx][\"loc\"][:3]; pos.append(loc)\n",
    "        else: pos.append(np.zeros(3))\n",
    "    pos=np.array(pos)\n",
    "    dists=np.linalg.norm(pos[:,None,:]-pos[None,:,:], axis=-1)\n",
    "    A=np.exp(-(dists**2)/(2.0*(sigma**2)+1e-12))\n",
    "    np.fill_diagonal(A, 1.0 if self_loop else 0.0)\n",
    "    D=np.diag(np.sum(A,axis=1)+1e-8)\n",
    "    D_inv_sqrt=np.linalg.inv(np.sqrt(D))\n",
    "    An=D_inv_sqrt @ A @ D_inv_sqrt\n",
    "    return A.astype(np.float32), An.astype(np.float32), dists.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54482e4f",
   "metadata": {},
   "source": [
    "## 7. Torch Dataset & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07c40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrialsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X=X.astype(np.float32); self.y=y.astype(np.int64)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return torch.from_numpy(self.X[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "def make_loaders(Xtr, ytr, Xte, yte, batch_size=BATCH_SIZE):\n",
    "    if SESSION_A_TRAIN_SESSION_B_TEST and Xte is not None:\n",
    "        X_train,y_train=Xtr,ytr\n",
    "        X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,test_size=VAL_SIZE,random_state=RANDOM_SEED,stratify=y_train)\n",
    "        X_test,y_test=Xte,yte\n",
    "    else:\n",
    "        X_train,X_temp,y_train,y_temp = train_test_split(Xtr,ytr,test_size=0.3,random_state=RANDOM_SEED,stratify=ytr)\n",
    "        X_val,X_test,y_val,y_test = train_test_split(X_temp,y_temp,test_size=0.5,random_state=RANDOM_SEED,stratify=y_temp)\n",
    "    print_counts(\"Train\", y_train); print_counts(\"Val  \", y_val); print_counts(\"Test \", y_test)\n",
    "    ds_tr,ds_va,ds_te = TrialsDataset(X_train,y_train), TrialsDataset(X_val,y_val), TrialsDataset(X_test,y_test)\n",
    "    dl_tr=DataLoader(ds_tr,batch_size=batch_size,shuffle=True,drop_last=False)\n",
    "    dl_va=DataLoader(ds_va,batch_size=batch_size,shuffle=False,drop_last=False)\n",
    "    dl_te=DataLoader(ds_te,batch_size=batch_size,shuffle=False,drop_last=False)\n",
    "    return dl_tr, dl_va, dl_te, (X_train.shape[1], X_train.shape[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b527b6a",
   "metadata": {},
   "source": [
    "## 8. Model — TemporalCNN + Channel Gating + GCN + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fbc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.logits=nn.Parameter(torch.zeros(n_channels))\n",
    "    def forward(self, x):\n",
    "        g=torch.sigmoid(self.logits); return x*g[None,:,None], g\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        super().__init__()\n",
    "        self.lin=nn.Linear(in_feats,out_feats,bias=bias); self.act=nn.ReLU(inplace=True)\n",
    "    def forward(self, H, A_norm):\n",
    "        HW=self.lin(H); out=torch.einsum('ij,bcj->bci', A_norm, HW); return self.act(out)\n",
    "\n",
    "class EEG_GCNNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, T, A_norm, gate_l1=0.0):\n",
    "        super().__init__()\n",
    "        self.n_channels=n_channels; self.register_buffer(\"A\", A_norm); self.gate_l1=gate_l1\n",
    "        self.grp_conv1=nn.Conv1d(n_channels, n_channels*8, kernel_size=25, padding=12, groups=n_channels)\n",
    "        self.grp_conv2=nn.Conv1d(n_channels*8, n_channels*8, kernel_size=9, padding=4, groups=n_channels)\n",
    "        self.grp_pool=nn.AdaptiveAvgPool1d(1); self.feat_dim=8\n",
    "        self.gate=ChannelGate(n_channels)\n",
    "        self.gcn1=GCNLayer(self.feat_dim,32); self.gcn2=GCNLayer(32,64)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        self.classifier=nn.Sequential(nn.Linear(64,64), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.Linear(64,n_classes))\n",
    "    def forward(self, x):\n",
    "        x,gates=self.gate(x)\n",
    "        H=F.relu(self.grp_conv1(x)); H=F.relu(self.grp_conv2(H)); H=self.grp_pool(H).squeeze(-1)\n",
    "        H=H.view(H.size(0), self.n_channels, self.feat_dim)\n",
    "        H=self.gcn1(H, self.A); H=self.gcn2(H, self.A); H=H.mean(dim=1)\n",
    "        out=self.classifier(self.dropout(H)); return out, gates\n",
    "\n",
    "def gate_l1_penalty(gates, coeff):\n",
    "    if coeff<=0: return torch.tensor(0., device=gates.device)\n",
    "    return coeff*torch.sum(torch.abs(gates))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f9073",
   "metadata": {},
   "source": [
    "## 9. Training & Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7317adba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=PATIENCE, min_delta=0.0):\n",
    "        self.patience=patience; self.min_delta=min_delta; self.counter=0; self.best=None; self.should_stop=False\n",
    "    def step(self, metric, model, path):\n",
    "        if (self.best is None) or (metric>self.best+self.min_delta):\n",
    "            self.best=metric; self.counter=0; torch.save(model.state_dict(), path)\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.counter>=self.patience: self.should_stop=True\n",
    "\n",
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train=optimizer is not None; model.train(is_train)\n",
    "    losses, yh, yt = [], [], []\n",
    "    for xb,yb in loader:\n",
    "        xb=xb.to(device); yb=yb.to(device)\n",
    "        if is_train: optimizer.zero_grad()\n",
    "        logits,gates=model(xb)\n",
    "        loss=F.cross_entropy(logits,yb)+gate_l1_penalty(gates, model.gate_l1)\n",
    "        if is_train:\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(),5.0); optimizer.step()\n",
    "        losses.append(loss.item()); yh.append(logits.detach().cpu().numpy().argmax(axis=1)); yt.append(yb.detach().cpu().numpy())\n",
    "    yh=np.concatenate(yh); yt=np.concatenate(yt)\n",
    "    acc=accuracy_score(yt,yh); f1m=f1_score(yt,yh,average='macro'); kappa=cohen_kappa_score(yt,yh)\n",
    "    return np.mean(losses), acc, f1m, kappa, yh, yt\n",
    "\n",
    "def evaluate(model, loader, class_names=CLASS_NAMES, title=\"Confusion Matrix\"):\n",
    "    model.eval(); yh_all, yt_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in loader:\n",
    "            xb=xb.to(device); yb=yb.to(device)\n",
    "            logits,_=model(xb); yh_all.append(logits.argmax(dim=1).cpu().numpy()); yt_all.append(yb.cpu().numpy())\n",
    "    yh=np.concatenate(yh_all); yt=np.concatenate(yt_all)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(yt,yh,target_names=class_names,digits=3))\n",
    "    cm=confusion_matrix(yt,yh,labels=list(range(len(class_names))))\n",
    "    fig=plt.figure(figsize=(6,5)); plt.imshow(cm, interpolation='nearest'); plt.title(title); plt.colorbar()\n",
    "    tick=np.arange(len(class_names)); plt.xticks(tick,class_names,rotation=45); plt.yticks(tick,class_names)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.tight_layout(); plt.show()\n",
    "    return cm, yt, yh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb2e673",
   "metadata": {},
   "source": [
    "## 10. End‑to‑End: Load, Build Graph, Train, Evaluate (one subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3911fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mne.events_from_annotations -> event_id: {np.str_('1023'): 1, np.str_('1072'): 2, np.str_('276'): 3, np.str_('277'): 4, np.str_('32766'): 5, np.str_('768'): 6, np.str_('769'): 7, np.str_('770'): 8, np.str_('771'): 9, np.str_('772'): 10}\n",
      "build_event_map: annotation keys (sample): ['1023', '1072', '276', '277', '32766', '768', '769', '770', '771', '772']\n",
      "Detected BCI2000(769..772) numeric mapping -> {7: 0, 8: 1, 9: 2, 10: 3}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m subject_id=\u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m Xtr,ytr,Xte,yte,CHS,sf = \u001b[43mload_subject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mShapes:\u001b[39m\u001b[33m\"\u001b[39m, Xtr.shape, ytr.shape, \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m Xte \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m Xte.shape)\n\u001b[32m      4\u001b[39m train_files,_=_find_subject_files(DATA_ROOT, subject_id)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mload_subject\u001b[39m\u001b[34m(data_root, subject_id)\u001b[39m\n\u001b[32m     67\u001b[39m     _get_montage_info(raw); _preprocess_raw(raw); rawT_list.append(raw)\n\u001b[32m     68\u001b[39m rawT=mne.concatenate_raws(rawT_list, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m Xtr,ytr,chs,sf=\u001b[43m_epochs_from_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m Xte=yte=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_files:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36m_epochs_from_raw\u001b[39m\u001b[34m(raw)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_epochs_from_raw\u001b[39m(raw):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     events, emap = \u001b[43m_map_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     picks = mne.pick_types(raw.info, eeg=\u001b[38;5;28;01mTrue\u001b[39;00m, meg=\u001b[38;5;28;01mFalse\u001b[39;00m, stim=\u001b[38;5;28;01mFalse\u001b[39;00m, eog=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     53\u001b[39m     ch_names = [raw.ch_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m picks]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36m_map_events\u001b[39m\u001b[34m(raw)\u001b[39m\n\u001b[32m     38\u001b[39m mapped_events = []\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ev \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     onset, _, code = \u001b[38;5;28mint\u001b[39m(ev[\u001b[32m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(ev[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mev\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(ev[\u001b[32m2\u001b[39m])\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m emap:\n\u001b[32m     42\u001b[39m         \u001b[38;5;66;03m# skip non-MI codes\u001b[39;00m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "subject_id=1\n",
    "Xtr,ytr,Xte,yte,CHS,sf = load_subject(DATA_ROOT, subject_id)\n",
    "print(\"Shapes:\", Xtr.shape, ytr.shape, None if Xte is None else Xte.shape)\n",
    "train_files,_=_find_subject_files(DATA_ROOT, subject_id)\n",
    "raw_tmp=mne.io.read_raw_gdf(sorted(train_files)[0], preload=False, verbose=False); _get_montage_info(raw_tmp)\n",
    "A,A_norm,dmat=build_adjacency(CHS, raw_tmp.info, sigma=0.08, self_loop=True); print(\"Adjacency:\", A_norm.shape)\n",
    "dl_tr,dl_va,dl_te,(C,T)=make_loaders(Xtr,ytr,Xte,yte,batch_size=BATCH_SIZE)\n",
    "A_t=torch.tensor(A_norm,dtype=torch.float32,device=device)\n",
    "model=EEG_GCNNet(n_channels=C,n_classes=NUM_CLASSES,T=T,A_norm=A_t,gate_l1=GATE_L1).to(device)\n",
    "opt=torch.optim.AdamW(model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,mode=\"max\",patience=LR_PATIENCE,factor=0.5,verbose=True)\n",
    "early=EarlyStopper(patience=PATIENCE,min_delta=1e-4); best_path=str(SAVE_DIR/f\"subject{subject_id:02d}_best.pt\")\n",
    "hist={\"train\":[],\"val\":[]}\n",
    "for epoch in range(1,N_EPOCHS+1):\n",
    "    tr_loss,tr_acc,tr_f1,tr_k,_,_=run_epoch(model,dl_tr,optimizer=opt)\n",
    "    va_loss,va_acc,va_f1,va_k,_,_=run_epoch(model,dl_va,optimizer=None)\n",
    "    scheduler.step(va_acc); early.step(va_acc, model, best_path)\n",
    "    hist[\"train\"].append([tr_loss,tr_acc,tr_f1,tr_k]); hist[\"val\"].append([va_loss,va_acc,va_f1,va_k])\n",
    "    print(f\"Epoch {epoch:03d} | Train L={tr_loss:.3f} A={tr_acc:.3f} F1={tr_f1:.3f} K={tr_k:.3f} || Val L={va_loss:.3f} A={va_acc:.3f} F1={va_f1:.3f} K={va_k:.3f} LR={opt.param_groups[0]['lr']:.2e}\")\n",
    "    if early.should_stop: print(\"Early stopping.\"); break\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "cm,yt,yh = evaluate(model, dl_te if dl_te is not None else dl_va, class_names=CLASS_NAMES, title=\"Confusion Matrix (Best)\")\n",
    "with torch.no_grad(): gates=torch.sigmoid(model.gate.logits).detach().cpu().numpy()\n",
    "topk=10; idx=np.argsort(-gates)[:topk]; print(\"\\nTop-%d channels by learned gate:\"%topk)\n",
    "for rank,i in enumerate(idx,1): print(f\"{rank:2d}. {CHS[i]}  (weight={gates[i]:.3f})\")\n",
    "plt.figure(figsize=(7,3)); plt.bar(np.arange(len(CHS)),gates); plt.xticks(np.arange(len(CHS)),CHS,rotation=90)\n",
    "plt.ylabel(\"Gate weight\"); plt.title(\"Per-channel importance (sigmoid gate)\"); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f3de0",
   "metadata": {},
   "source": [
    "## 11. (Optional) Loop over subjects & report mean metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d475833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_subject(sid):\n",
    "    Xtr,ytr,Xte,yte,CHS,sf=load_subject(DATA_ROOT,sid)\n",
    "    train_files,_=_find_subject_files(DATA_ROOT,sid)\n",
    "    raw_tmp=mne.io.read_raw_gdf(sorted(train_files)[0], preload=False, verbose=False); _get_montage_info(raw_tmp)\n",
    "    A,A_norm,_=build_adjacency(CHS, raw_tmp.info, sigma=0.08, self_loop=True)\n",
    "    dl_tr,dl_va,dl_te,(C,T)=make_loaders(Xtr,ytr,Xte,yte,batch_size=BATCH_SIZE)\n",
    "    A_t=torch.tensor(A_norm,dtype=torch.float32,device=device)\n",
    "    model=EEG_GCNNet(n_channels=C,n_classes=NUM_CLASSES,T=T,A_norm=A_t,gate_l1=GATE_L1).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=LR,weight_decay=WEIGHT_DECAY)\n",
    "    scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(opt,mode=\"max\",patience=LR_PATIENCE,factor=0.5,verbose=False)\n",
    "    early=EarlyStopper(patience=PATIENCE,min_delta=1e-4); best_path=str(SAVE_DIR/f\"subject{sid:02d}_best.pt\")\n",
    "    for epoch in range(1,N_EPOCHS+1):\n",
    "        tr_loss,tr_acc,tr_f1,tr_k,_,_=run_epoch(model,dl_tr,optimizer=opt)\n",
    "        va_loss,va_acc,va_f1,va_k,_,_=run_epoch(model,dl_va,optimizer=None)\n",
    "        scheduler.step(va_acc); early.step(va_acc, model, best_path)\n",
    "        if early.should_stop: break\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    _,yt,yh=evaluate(model, dl_te if dl_te is not None else dl_va, class_names=CLASS_NAMES, title=f\"Subject {sid:02d} CM\")\n",
    "    acc=accuracy_score(yt,yh); f1m=f1_score(yt,yh,average='macro'); k=cohen_kappa_score(yt,yh)\n",
    "    return acc,f1m,k\n",
    "# Example:\n",
    "# results=[train_one_subject(s) for s in range(1,10)]\n",
    "# print(\"Mean acc=%.3f, f1=%.3f, kappa=%.3f\"%tuple(np.mean(results,axis=0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109e25b",
   "metadata": {},
   "source": [
    "## 12. Troubleshooting Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae01b9f",
   "metadata": {},
   "source": [
    "\n",
    "- **Label sanity:** print `np.bincount(y, minlength=4)` right after epoching. Each class should have similar counts.\n",
    "- **Shapes:** Make sure inputs are `(batch, channels, time)` and final logits are `(batch, 4)`.\n",
    "- **Learning rate:** If accuracy stagnates at a single class, try lowering LR to `3e-4` or increasing `N_EPOCHS`.\n",
    "- **Artifact rejection:** Heavy artifacts can poison features. Consider removing bad trials with MNE’s autoreject or simple amplitude thresholds.\n",
    "- **Window:** You can tweak `TMIN/TMAX` to 0.5–2.5s or 1.0–3.0s after cue onset; small changes can help.\n",
    "- **Graph:** `sigma` in `build_adjacency` controls the spatial neighborhood. Try `0.06..0.12`.\n",
    "- **Channel selection:** Increase `GATE_L1` to push more sparsity; inspect the top‑k list for neuro‑plausible channels (C3/Cz/C4/CPz/FCz, etc.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
